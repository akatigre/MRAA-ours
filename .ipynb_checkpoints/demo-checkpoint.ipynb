{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AliaksandrSiarohin/articulated-animation/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FPGt7_2z7fn"
   },
   "source": [
    "\n",
    "# Demo for paper \"Motion Representations for Articulated Animation\"\n",
    "**Clone repository. First install git lfs. Pulling all checkpoints may take a while**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YinzvwYd0MWj"
   },
   "source": [
    "**Load source image and driving video.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "CRVOnlASzrhI",
    "outputId": "02896e4c-3530-4db2-b415-be4c2ae72066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3) (384, 384, 3)\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from skimage.transform import resize\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "\n",
    "# from re_demo import load_checkpoints\n",
    "\n",
    "import sys\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte, img_as_float32\n",
    "import torch\n",
    "from sync_batchnorm import DataParallelWithCallback\n",
    "from modules.generator import Generator\n",
    "from modules.region_predictor import RegionPredictor\n",
    "from modules.avd_network import AVDNetwork\n",
    "from animate import get_animation_region_params\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import pickle as pkl\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import cv2\n",
    "\n",
    "def to_structure(img):\n",
    "    img = img[..., ::-1] # RGB -> BGR\n",
    "    img = cv2.resize(img, (128, 128))\n",
    "    img = cv2.edgePreservingFilter(img, flags=cv2.RECURS_FILTER, sigma_s=100, sigma_r=0.7)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGR -> RGB\n",
    "    # structure_images.append(img_as_float32(img))\n",
    "    return img_as_float32(img)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "dataset = 'ted'\n",
    "source_name = f\"{dataset}_test/test_picture1.png\"\n",
    "source_image = imageio.imread(source_name)\n",
    "# resize(, (1024, 1024))[..., :3]\n",
    "\n",
    "driving_video = imageio.mimread(f'{dataset}_test/test_video.mp4')\n",
    "\n",
    "video_shape = (384, 384) if dataset=='ted' else (256, 256)\n",
    "# video_shape = (64, 64)\n",
    "source_image256 = resize(source_image, video_shape)[..., :3]\n",
    "source_structure = to_structure(source_image)\n",
    "\n",
    "# driving_video = [resize(frame, video_shape)[..., :3] for frame in driving_video]\n",
    "driving_video = [to_structure(frame) for frame in driving_video]\n",
    "print(source_image.shape, source_image256.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n49-tAcEzzeG"
   },
   "outputs": [],
   "source": [
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    raise Exception(\"You must use Python 3 or higher. Recommended version is Python 3.7\")\n",
    "    \n",
    "def load_checkpoints(config_path, checkpoint_path, cpu=False):\n",
    "    with open(config_path) as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    generator = Generator(num_regions=config['model_params']['num_regions'],\n",
    "                          num_channels=config['model_params']['num_channels'],\n",
    "                          **config['model_params']['generator_params'])\n",
    "    if not cpu:\n",
    "        generator.cuda()\n",
    "\n",
    "    region_predictor = RegionPredictor(num_regions=config['model_params']['num_regions'],\n",
    "                                       num_channels=config['model_params']['num_channels'],\n",
    "                                       estimate_affine=config['model_params']['estimate_affine'],\n",
    "                                       **config['model_params']['region_predictor_params'])\n",
    "    \n",
    "    if not cpu:\n",
    "        region_predictor.cuda()\n",
    "\n",
    "    avd_network = AVDNetwork(num_regions=config['model_params']['num_regions'],\n",
    "                             **config['model_params']['avd_network_params'])\n",
    "    if not cpu:\n",
    "        avd_network.cuda()\n",
    "\n",
    "    if cpu:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "\n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    region_predictor.load_state_dict(checkpoint['region_predictor'])\n",
    "    if 'avd_network' in checkpoint:\n",
    "        avd_network.load_state_dict(checkpoint['avd_network'])\n",
    "\n",
    "    if not cpu:\n",
    "        generator = DataParallelWithCallback(generator)\n",
    "        region_predictor = DataParallelWithCallback(region_predictor)\n",
    "        avd_network = DataParallelWithCallback(avd_network)\n",
    "\n",
    "    generator.eval()\n",
    "    region_predictor.eval()\n",
    "    avd_network.eval()\n",
    "\n",
    "    return generator, region_predictor, avd_network\n",
    "\n",
    "config_path = 'config/ted_structure128_img384.yaml'\n",
    "checkpoint_path = 'log/ted_structure128_img384/00000026-cpk-reconstruction.pth'\n",
    "generator, region_predictor, avd_network = load_checkpoints(config_path=config_path,\n",
    "                                                            checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wP-o3LQq0a79"
   },
   "source": [
    "**Perform animation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb 셀 7\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver29.mli.kr/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     ax\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver29.mli.kr/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m image\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bserver29.mli.kr/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m source_region_params, driving_region_params \u001b[39m=\u001b[39m make_animation(source_image, driving_video, generator, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver29.mli.kr/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m                              region_predictor, avd_network, animation_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mavd\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver29.mli.kr/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(nrows\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, ncols\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m, \u001b[39m15\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver29.mli.kr/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m draw_image_with_kp(driving_video[\u001b[39m0\u001b[39m], axs[\u001b[39m0\u001b[39m], driving_region_params[\u001b[39m'\u001b[39m\u001b[39mshift\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), kp_size\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[1;32m/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb 셀 7\u001b[0m in \u001b[0;36mmake_animation\u001b[0;34m(source_image, driving_video, generator, region_predictor, avd_network, animation_mode, cpu)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver29.mli.kr/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         source \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver29.mli.kr/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     driving \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray(driving_video)[np\u001b[39m.\u001b[39mnewaxis]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32))\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bserver29.mli.kr/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     source_region_params \u001b[39m=\u001b[39m region_predictor(source)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver29.mli.kr/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     driving_region_params \u001b[39m=\u001b[39m region_predictor(driving[:, :, \u001b[39m0\u001b[39m]) \u001b[39m# First frame\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bserver29.mli.kr/home/server29/jonghoon_workspace/MRAA-ours/demo.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m source_region_params, driving_region_params\n",
      "File \u001b[0;32m~/anaconda3/envs/thinPlate/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/thinPlate/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:154\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbuffers()):\n\u001b[1;32m    153\u001b[0m     \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj:\n\u001b[0;32m--> 154\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule must have its parameters and buffers \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mon device \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m (device_ids[0]) but found one of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mthem on device: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj, t\u001b[39m.\u001b[39mdevice))\n\u001b[1;32m    158\u001b[0m inputs, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscatter(inputs, kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids)\n\u001b[1;32m    159\u001b[0m \u001b[39m# for forward function without any inputs, empty list and dict will be created\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m# so the module can be executed on one device which is the first one in device_ids\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:1"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "\n",
    "def make_animation(source_image, driving_video, generator, region_predictor, avd_network,\n",
    "                   animation_mode='standard', cpu=False):\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            source = source.cuda()\n",
    "        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "        source_region_params = region_predictor(source)\n",
    "        driving_region_params = region_predictor(driving[:, :, 0]) # First frame\n",
    "\n",
    "    return source_region_params, driving_region_params\n",
    "\n",
    "\n",
    "from skimage.draw import circle\n",
    "\n",
    "kp_size = 5\n",
    "draw_border = True\n",
    "colormap = plt.get_cmap('gist_rainbow')\n",
    "region_bg_color = [1, 1, 1]\n",
    "\n",
    "    \n",
    "def draw_image_with_kp(image, ax, kp_array, kp_size):\n",
    "    \n",
    "    image = np.copy(image)\n",
    "    spatial_size = np.array(image.shape[:2][::-1])[np.newaxis]\n",
    "    kp_array = spatial_size * (kp_array + 1) / 2\n",
    "    num_regions = kp_array.shape[0]\n",
    "    for kp_ind, kp in enumerate(kp_array):\n",
    "        rr, cc = disk((kp[1], kp[0]), kp_size, shape=image.shape[:2])\n",
    "        ax.text(kp[0], kp[1], f\"{kp_ind}\", dict(color='red', va='center', ha='center'))\n",
    "        image[rr, cc] = np.array(colormap(kp_ind / num_regions))[:3]\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    return image\n",
    "\n",
    "source_region_params, driving_region_params = make_animation(source_image, driving_video, generator, \n",
    "                             region_predictor, avd_network, animation_mode='avd')\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 15))\n",
    "draw_image_with_kp(driving_video[0], axs[0], driving_region_params['shift'][0].data.cpu().numpy(), kp_size=5)\n",
    "draw_image_with_kp(source_image256, axs[1], source_region_params['shift'][0].data.cpu().numpy(), kp_size=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optical Flow Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms \n",
    "from PIL import ImageFont, ImageDraw\n",
    "\n",
    "\n",
    "    \n",
    "def add_annotation(img, annotation, font_scale):\n",
    "    if font_scale:\n",
    "        scale = 4\n",
    "    else:\n",
    "        scale = 1\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", int(60//scale)) # default font directory of ubuntu\n",
    "    img = torch.Tensor(img)\n",
    "    img = img.permute(2, 0, 1) if img.shape[0]!=3 else img\n",
    "\n",
    "    img_ = transforms.ToPILImage()(img) # C, H, W\n",
    "    draw = ImageDraw.Draw(img_)\n",
    "    draw.text((60/scale, 30/scale), annotation, font=font, fill=(190, 80, 120, 255))\n",
    "    img_ = transforms.ToTensor()(img_) # C, H, W\n",
    "    return img_.numpy().transpose(1, 2, 0) # C, H, W\n",
    "\n",
    "\n",
    "def optical_flow_ang(flow, occlusion, font_scale, shape=(256, 256, 3)):\n",
    "    hsv = np.zeros(shape, dtype=np.uint8)\n",
    "    hsv[..., 1] = 255\n",
    "    flow = flow.squeeze(0).permute(2, 0, 1).numpy()\n",
    "    occlusion = occlusion.squeeze(0)\n",
    "    occlusion = np.concatenate((occlusion, occlusion, occlusion), axis=0)\n",
    "    # scale = shape[0]/flow.shape[1]\n",
    "    # if scale!=1:\n",
    "    #     flow = flow.permute(0, 3, 1, 2)\n",
    "    #     flow = F.interpolate(flow, scale_factor=scale, mode='bilinear').squeeze(0).numpy()\n",
    "    #     occlusion = F.interpolate(occlusion, scale_factor=scale, mode='bilinear').squeeze(0)\n",
    "    #     occlusion = occlusion.permute(1, 2, 0).numpy()\n",
    "    #     \n",
    "    mag, ang = cv2.cartToPolar(flow[0, ...], flow[1, ...])\n",
    "    hsv[..., 0] = ang * 180/ np.pi / 2\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    of = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    occlusion = add_annotation(occlusion, 'occlusion mask', font_scale=font_scale) # C, H, W\n",
    "    return of, occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def make_animation(source_image256, source_image512, driving_video, generator, region_predictor, avd_network, dataset, scale,\n",
    "                   animation_mode='standard', ground_truth=None, cpu=False, keypoints=None):\n",
    "\n",
    "    out = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "\n",
    "        # Compute keypoints, affine transformation from 256 resolution\n",
    "        source_image256 = source_image256\n",
    "        source256 = torch.tensor(source_image256[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            source256 = source256.cuda()\n",
    "        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
    "\n",
    "        # TODO: Add keypoints manually\n",
    "        if keypoints is not None:\n",
    "            pass\n",
    "        else:\n",
    "            keypoints = None\n",
    "\n",
    "        source_region_params = region_predictor(source256, keypoints)\n",
    "        driving_region_params_initial = region_predictor(driving[:, :, 0])\n",
    "\n",
    "        source512 = torch.tensor(source_image512[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
    "        if not cpu:\n",
    "            source512 = source512.cuda()\n",
    "        for frame_idx in tqdm(range(driving.shape[2])):\n",
    "            driving_frame = driving[:, :, frame_idx]\n",
    "            if not cpu:\n",
    "                driving_frame = driving_frame.cuda()\n",
    "            driving_region_params = region_predictor(driving_frame)\n",
    "            \n",
    "            new_region_params = get_animation_region_params(source_region_params, driving_region_params,\n",
    "                                                            driving_region_params_initial, avd_network=avd_network,\n",
    "                                                            mode=animation_mode)\n",
    "            \n",
    "            out = generator(source256, source512, source_region_params=source_region_params, driving_region_params=new_region_params)\n",
    "            # deformed: torch.Size([1, 3, 1024, 1024])\n",
    "            # optical_flow: torch.Size([1, 1024, 1024, 2])\n",
    "            # occlusion_map: torch.Size([1, 1, 1024, 1024])\n",
    "            # prediction: torch.Size([1, 3, 1024, 1024])\n",
    "            res = out['prediction'].shape[-1]\n",
    "            of, om = optical_flow_ang(out['optical_flow'].detach().cpu(), out['occlusion_map'].detach().cpu(), font_scale=scale, shape=(res, res, 3))\n",
    "            \n",
    "            deformed = add_annotation(out['deformed'].cpu()[0], 'deformed source', font_scale=scale)\n",
    "            pred = add_annotation(out['prediction'].cpu()[0], 'inpainted', font_scale=scale)\n",
    "            # deformed = np.transpose(out['deformed'].data.cpu().numpy(), [0, 2, 3, 1])[0]\n",
    "            # pred = np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0]\n",
    "            predictions.append(np.concatenate([deformed, pred, om, of/255], axis=1))\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Trained with (384, 384, 3), Infer with (384, 384, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:30<00:00,  5.18it/s]\n"
     ]
    }
   ],
   "source": [
    "original = source_image256\n",
    "hr = source_image256\n",
    "scale = (hr.shape[0]/original.shape[0])**2\n",
    "if int(scale)==1:\n",
    "        scale=True\n",
    "print(scale)\n",
    "print(f\"Trained with {original.shape}, Infer with {hr.shape}\")\n",
    "predictions = make_animation(original, hr,\n",
    "        driving_video, generator, region_predictor, \n",
    "        avd_network, dataset, scale=scale, animation_mode='avd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_name =f'{dataset}_orig{original.shape[0]}_hr{hr.shape[0]}.mp4'\n",
    "imageio.mimsave(vid_name, [(frame*255).astype(np.uint8) for frame in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import imageio\n",
    "import sys \n",
    "sys.path.append(\"liif\")\n",
    "import models\n",
    "from utils import make_coord\n",
    "from liif.test import batched_predict\n",
    "dataset = 'ted'\n",
    "\n",
    "vid_name = f'{dataset}_orig384_hr384.mp4'\n",
    "\n",
    "reader = imageio.get_reader(vid_name)\n",
    "for frame in reader:\n",
    "    img = transforms.ToTensor()(frame)[...,384:2*384]\n",
    "    transforms.ToPILImage()(img).save('orig.png')\n",
    "    model = models.make(torch.load('liif/edsr-baseline-liif.pth')['model'], load_sd=True).cuda()\n",
    "    h, w = 1024, 1024\n",
    "    coord = make_coord((h, w)).cuda()\n",
    "    cell = torch.ones_like(coord)\n",
    "    cell[:, 0] *= 2 / h\n",
    "    cell[:, 1] *= 2 / w\n",
    "    pred = batched_predict(model, ((img - 0.5) / 0.5).cuda().unsqueeze(0),\n",
    "        coord.unsqueeze(0), cell.unsqueeze(0), bsize=30000)[0]\n",
    "    pred = (pred * 0.5 + 0.5).clamp(0, 1).view(h, w, 3).permute(2, 0, 1).cpu()\n",
    "    transforms.ToPILImage()(pred).save('tmp.png')\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDGE Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from canny_edge_detector import canny_edge_detector as ced\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "  \n",
    "# Detecting Edges on the Image using the argument ImageFilter.FIND_EDGES\n",
    "image = image.filter(ImageFilter.FIND_EDGES)\n",
    "img_np = np.array(image)\n",
    "# Saving the Image Under the name Edge_Sample.png\n",
    "image.save(r\"Edge_Sample.png\")\n",
    "image = Image.open(source_name)\n",
    "image = image.convert(\"L\")\n",
    "image_np = np.array(image)\n",
    "print(image_np.shape)\n",
    "detector = ced.cannyEdgeDetector([image_np], sigma=1.4, kernel_size=5, lowthreshold=0.1, highthreshold=0.1, weak_pixel=10)\n",
    "imgs_final = detector.detect()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNSylbAA8rEY4SPEXcQH+18",
   "include_colab_link": true,
   "name": "demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('thinPlate')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8b0b0bf0646a5743fcb35c430d8758cb53e3f39c00608307fee24efd20e7910"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
